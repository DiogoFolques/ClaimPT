# -*- coding: utf-8 -*-
"""Copy of IAA - CLAIMPT - Improved

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sE2wbPGWeKwwo8niYlQC01jRZJMKSFGR
"""

!pip install krippendorff

import pandas as pd
import numpy as np
import krippendorff

from google.colab import drive
drive.mount('/content/drive')

"""#Definir Variáveis e Funções"""

import os
folder=os.path.join("/","content","drive","MyDrive","ClaimPT")
attributes_folder=os.path.join(folder,"attributes")

#mapping between files and annotators

dict_files={
          "relations.csv": ["Raquel Sequeira", "Sara Silva"],
          "spans.csv": ["Raquel Sequeira", "Sara Silva"],
          "span_label.csv": ["Raquel Sequeira", "Sara Silva"]
}






# print(dict_files)

#same for attribute files

dict_files_attrs=dict()
dict_files_attrs = {
    "claimer_attribute.csv": ["Raquel Sequeira", "Sara Silva"],
    "metadata.csv": ["Raquel Sequeira", "Sara Silva"],
    "news_article_topic.csv": ["Raquel Sequeira", "Sara Silva"],
    "stance.csv": ["Raquel Sequeira", "Sara Silva"],
    "topic.csv": ["Raquel Sequeira", "Sara Silva"]
}







print(dict_files_attrs)


###  UNSTACK  ###

def unstack_claim_span(df):
    """
    Split STACKED rows into multiple SPLIT rows, pairing single-label sides with all multi-labels.
    """
    A = next(c for c in df.columns if 'Raquel' in c)
    B = next(c for c in df.columns if 'Sara' in c)
    missing = {'<no annotation>', '<no label>'}

    # Split normal vs stacked
    mask = df['Flags'].str.upper().str.contains(r'\bSTACKED\b', na=False)
    normal  = df[~mask].copy()
    stacked = df[mask].copy()

    rows = []
    for _, r in stacked.iterrows():
        # Clean lists of non‑missing labels
        a_list = [lbl.strip() for lbl in str(r[A]).split(',') if lbl.strip() and lbl.strip() not in missing]
        b_list = [lbl.strip() for lbl in str(r[B]).split(',') if lbl.strip() and lbl.strip() not in missing]

        # Determine how many splits
        max_len = max(len(a_list), len(b_list), 1)

        # If empty, fill with '<no annotation>'
        if not a_list:
            a_list = ['<no annotation>'] * max_len
        if not b_list:
            b_list = ['<no annotation>'] * max_len

        # If one side has a single label and other multiple, repeat the single
        if len(a_list) == 1 and len(b_list) > 1:
            a_list *= len(b_list)
        elif len(b_list) == 1 and len(a_list) > 1:
            b_list *= len(a_list)

        # Emit one SPLIT row per paired label
        for a_lbl, b_lbl in zip(a_list, b_list):
            new_row = r.copy()
            new_row[A] = a_lbl
            new_row[B] = b_lbl
            new_row['Flags'] = 'SPLIT'
            rows.append(new_row)

    # Combine original non‑stacked and all new split rows
    return pd.concat([normal, pd.DataFrame(rows)], ignore_index=True)


### MERGE ROWS WITH RELAX METRICS  ###

import re

def merge_relaxed_annotations(
    df,
    file_col='Document',
    pos_col='Position',
    annotator_cols=('Raquel Sequeira', 'Sara Silva'),
    char_tol=4
):
    """
    Within each file, cluster any rows whose spans overlap within `char_tol` chars.
    - Drops rows where both annotators are missing ('<no annotation>' / '<no label>')
    - Merges each cluster into one row:
        * span = [min begin]–[max end] + original text
        * for each annotator: collect its non-missing labels
        * Flags = 'STACKED' if an annotator ends up with >1 label else 'MERGED'
    - Final pass: any row whose Raquel or Sara column has a comma forces Flags='STACKED'
    """
    A, B = annotator_cols
    missing = {"<no annotation>", "<no label>"}

    df = df.copy()
    # parse numeric begin/end
    spans = df[pos_col].str.extract(r"(\d+)-(\d+)")
    df['span_begin'] = spans[0].astype(int)
    df['span_end']   = spans[1].astype(int)

    # drop rows both missing
    both_missing = df[A].isin(missing) & df[B].isin(missing)
    df = df.loc[~both_missing].reset_index(drop=True)

    # union-find setup
    parent = list(range(len(df)))
    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    def union(x,y):
        rx, ry = find(x), find(y)
        if rx != ry:
            parent[ry] = rx

    def relax(r1, r2):
        # must overlap at all
        if min(r1['span_end'], r2['span_end']) <= max(r1['span_begin'], r2['span_begin']):
            return False
        # both boundaries within tol
        return (abs(r1['span_begin'] - r2['span_begin']) <= char_tol and
                abs(r1['span_end']   - r2['span_end'])   <= char_tol)

    # cluster by document
    for fname, idxs in df.groupby(file_col).groups.items():
        idxs = list(idxs)
        for i in range(len(idxs)):
            for j in range(i+1, len(idxs)):
                ix, jx = idxs[i], idxs[j]
                if relax(df.loc[ix], df.loc[jx]):
                    union(ix, jx)

    # gather clusters
    clusters = {}
    for idx in range(len(df)):
        root = find(idx)
        clusters.setdefault(root, []).append(idx)

    merged = []
    drop = []
    for root, members in clusters.items():
        if len(members) == 1:
            continue
        sub = df.loc[members]
        # new span edges
        b = sub['span_begin'].min()
        e = sub['span_end'].max()
        # extract original text inside [ ]
        txt = re.search(r"\[(.*)\]", sub.iloc[0][pos_col]).group(1)
        new_pos = f"{b}-{e} [{txt}]"
        # collect each annotator’s labels
        labsA = {L for L in sub[A].unique() if L not in missing}
        labsB = {L for L in sub[B].unique() if L not in missing}
        # flag
        flag = 'STACKED' if len(labsA)>1 or len(labsB)>1 else 'MERGED'
        # base row
        row = sub.iloc[0].copy()
        row[pos_col] = new_pos
        row[A] = ", ".join(sorted(labsA)) if labsA else "<no annotation>"
        row[B] = ", ".join(sorted(labsB)) if labsB else "<no annotation>"
        row['Flags'] = flag

        merged.append(row)
        drop.extend(members)

    # drop originals and append merges
    out = df.drop(index=drop).reset_index(drop=True)
    if merged:
        out = pd.concat([out, pd.DataFrame(merged)], ignore_index=True)

    # final check: if any annotator col has comma, force STACKED
    def final_flag(r):
        if ',' in r[A] or ',' in r[B]:
            return 'STACKED'
        return r['Flags']
    out['Flags'] = out.apply(final_flag, axis=1)

    # clean up
    return out.drop(columns=['span_begin','span_end'])




#loads dataset based on filename. attribute is boolean to know where the file is
def getDataandAnn(filename,attribute):
  if(attribute):
     df = pd.read_csv(os.path.join(attributes_folder,filename), dtype=str, keep_default_na=False)
     os.path.join(folder,filename)
     return(df,dict_files_attrs[filename])


  df = pd.read_csv(os.path.join(folder,filename), dtype=str, keep_default_na=False)
  df=merge_relaxed_annotations(df)
  df=unstack_claim_span(df)
  return(df,dict_files[filename])



#the id of the annotators as is in Inception ("Curator","Inês Cantante","Ana Filipa Pacheco")

#test if is working
for f in os.listdir(attributes_folder):
  print(f)
  if(f.endswith(".csv")):
    df,ann_ids=getDataandAnn(f,attribute=True)
    print(ann_ids)

#dataset example
df

"""#Spans"""

from sklearn.preprocessing import LabelEncoder

import numpy as np
def encode_annotations(df,ann_columns):
    """
    Encodes string annotations into numeric values.

    Parameters:
    - annotations: 2D list where rows = annotators and columns = items.

    Returns:
    - Encoded numpy array where unique labels are mapped to numbers.
    - Label mapping dictionary (for reference).
    """
    annotators_data= df[ann_columns]
    unique_labels=list()
    for ann in ann_columns:
      unique_labels=unique_labels+list(df[ann])

    unique_labels=list(set(sorted(unique_labels)))
    print(unique_labels)
    # Flatten and find unique labels
    #print(unique_labels)
    # Create label encoder
    label_encoder = LabelEncoder()
    label_encoder.fit(unique_labels)
    print("Encoded classes:", label_encoder.classes_)
    #print(unique_labels)
    # Encode annotations
    annotations_encoded=list()
    for ann in ann_columns:
      ann_list_enc=label_encoder.transform(list(df[ann].values))
      #this has to be improved
      if("<no annotation>" in unique_labels):
        ann_list_enc=[v if v!=0 else None for v in ann_list_enc]

      annotations_encoded.append(ann_list_enc)


    encoded_annotations = np.array( annotations_encoded, dtype=np.float64)
    #print(encoded_annotations)
    return encoded_annotations, label_encoder.classes_

"""## Kohen and Krippendorff Alpha"""

from sklearn.metrics import cohen_kappa_score

#comput krippendorff and cohen based on the encoded annotations

def computeKrippendorff_Cohen(filename,df,ann_ids,attributes=True):

  if(attributes):
    df = df[~((df[ann_ids[0]] == "<no label>") & (df[ann_ids[1]] == "<no label>"))]
    df = df[~((df[ann_ids[0]] == "<no label>") & (df[ann_ids[1]] == "<no annotation>"))]
    df = df[~((df[ann_ids[0]] == "<no annotation>") & (df[ann_ids[1]] == "<no label>"))]
    #if filename=="TemporalFunction_filipa_curador.csv":
    #  print(df)

  encoded_annotations,labels=encode_annotations(df,ann_ids)
  #print(labels)
  #garantee that aggreement between no labels are excluded

  try:
    k=krippendorff.alpha(reliability_data=encoded_annotations,level_of_measurement="nominal")
  except Exception as e:
    print(str(e))
    k=0
  #cohen does not accept missing (i.e NONE labels). filter these out
  df_complete = df[df["Flags"].isin(['COMPLETE, USED', 'DIFFERENCE, COMPLETE, USED'])]
  c=cohen_kappa_score(df_complete[ann_ids[0]],df_complete[ann_ids[1]])
  res=dict()
  res["filename"]=filename
  res["type"]=filename.split("_")[0]
  res["annotators"]=ann_ids
  res["krippendorff_alpha"]=k
  res["cohen_kappa"]=c
  return res

def computeKrippendorff_Cohen4all(output_filename):
  #run the datasets and compute the metrics for all files
  result=list()
  for f in os.listdir(attributes_folder):
    #attributes first
    print(f)

    if(f.endswith(".csv")):
      df,ann_ids=getDataandAnn(f,attribute=True)
      t=computeKrippendorff_Cohen(f,df,ann_ids)
      result.append(t)
  #span and relations after
  for f in os.listdir(folder):
    print(f)
    if(f.endswith(".csv")):
      df,ann_ids=getDataandAnn(f,attribute=False)
      t=computeKrippendorff_Cohen(f,df,ann_ids)
      result.append(t)

  result_df=pd.DataFrame.from_dict(result)
  result_df.to_csv(output_filename)

computeKrippendorff_Cohen4all("result_aggreement_spans.csv")

def simple_span_agreement(df, ann_ids, positive_label="CLAIM"):
    """
    Compute %‐agreement on the “positive” class (e.g. CLAIM)
    by counting intersection/union of spans.

    df: DataFrame with columns ann_ids[0], ann_ids[1], and a unique span identifier col “span_id”
    ann_ids: [annotator1_col, annotator2_col]
    positive_label: the label you treat as “span present” (e.g. “CLAIM”)
    """

    # build sets of span‑IDs each annotator marked positive
    set1 = set(df.loc[df[ann_ids[0]] == positive_label, "span_id"])
    set2 = set(df.loc[df[ann_ids[1]] == positive_label, "span_id"])

    intersection = set1 & set2
    union        = set1 | set2

    if not union:
        return None  # no positive spans at all

    agreement_pct = len(intersection) / len(union) * 100
    return {
        "both_positive":   len(intersection),
        "either_positive": len(union),
        "percent_agree":   agreement_pct
    }

# Example usage on your span.csv:
df, ann_ids = getDataandAnn("span_label.csv", attribute=False)

# You need a column that uniquely identifies each span row.
# If you don’t already have one, you can create it:
df = df.reset_index().rename(columns={"index":"span_id"})

res = simple_span_agreement(df, ann_ids, positive_label="Claim")  # or whatever your “span” label is
print(res)

"""#Visualizations"""

df,ann_ids=getDataandAnn("span_label.csv",attribute=False)

print("Any STACKED left? →", (df.Flags == "STACKED").sum())
print("Any bundled cells left? →",
      df[ann_ids[0]].str.contains(",").sum(), "rows with commas in Raquel’s column")
print("Any bundled cells left? →",
      df[ann_ids[1]].str.contains(",").sum(), "rows with commas in Sara’s column")

df[ann_ids]

from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
plt.subplots(figsize=(10, 10))

df = df[~((df[ann_ids[0]] == "<no label>") & (df[ann_ids[1]] == "<no label>"))]
df = df[~((df[ann_ids[0]] == "<no label>") & (df[ann_ids[1]] == "<no annotation>"))]
df = df[~((df[ann_ids[0]] == "<no annotation>") & (df[ann_ids[1]] == "<no label>"))]

df_2dhist = pd.DataFrame({
    x_label: grp['Sara Silva'].value_counts()
    for x_label, grp in df.groupby('Raquel Sequeira')
})

ax=sns.heatmap(df_2dhist,
               cmap='viridis',
               annot=True,
               annot_kws={"size": 14, "color": "white"}
               )
ax.set_xlabel("Raquel Sequeira",fontsize=12)
ax.set_ylabel("Sara Silva",fontsize=12)
ax.tick_params(axis='both', labelsize=12)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=12)


plt.show()

import krippendorff
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def krippendorff_without_non_claims(long_df, show_heatmap=True):
    """
    Filters out units where the majority label is 0 (Non-claim),
    computes Krippendorff's alpha, and optionally displays a heatmap.

    Args:
        long_df (pd.DataFrame): Must contain 'unit', 'annotator', 'label' columns.
        show_heatmap (bool): Whether to display a heatmap of agreement.

    Returns:
        alpha (float): Krippendorff's alpha for the filtered data.
    """
    # Pivot to wide format
    wide_df = long_df.pivot(index='unit', columns='annotator', values='label')

    # Drop units where the majority label is 0
    def majority(row):
        return row.mode().iloc[0] if not row.isnull().all() else None

    majority_labels = wide_df.apply(majority, axis=1)
    filtered_df = wide_df[majority_labels != 0]

    # Prepare matrix for krippendorff
    matrix = filtered_df.to_numpy().T  # shape: annotators x units

    # Compute Krippendorff's alpha
    alpha = krippendorff.alpha(reliability_data=matrix, level_of_measurement='nominal')
    print(f"Krippendorff's alpha (w/o non-claims): {alpha:.3f}")

    # Heatmap
    if show_heatmap:
        agreement_matrix = 1 - filtered_df.apply(lambda x: abs(x[:, None] - x), axis=1).mean()
        sns.heatmap(agreement_matrix, annot=True, cmap='coolwarm')
        plt.title("Inter-Annotator Agreement Heatmap (w/o Non-Claims)")
        plt.show()

    return alpha

import matplotlib.pyplot as plt

import seaborn as sns

def agreement_plot(df,ann_columns):
  agg=list()

  for ind,row in df.iterrows():
    list_values_ann = [row[col] for col in ann_columns]
    if all(val == list_values_ann[0] for val in list_values_ann):
      agg.append(row[ann_columns[0]])
    else:
      agg.append("No Aggreement")

  df["agg"]=agg
  plt.figure(figsize=(10, 6))  # Adjust width (10) and height (6) as needed

  ax=sns.countplot(df, x="agg")

  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha="right")

  # Rename x-axis label
  ax.set_xlabel("Agreement")

  # Convert y-axis to percentage
  total = len(df)
  for p in ax.patches:
      height = p.get_height()
      percentage = 100 * height / total
      ax.annotate(f'{percentage:.1f}%', (p.get_x() + p.get_width() / 2., height),
                  ha='center', va='bottom', fontsize=10, color='black')

  # Set y-axis labels as percentages
  ax.set_ylabel("Percentage")
  ax.set_yticklabels([f"{int(tick/total*100)}%" for tick in ax.get_yticks()])
  plt.show()

attribute_files = [f for f in os.listdir(attributes_folder) if f.endswith(".csv")]
annotators_id   = ["Raquel Sequeira", "Sara Silva"]


for f in attribute_files:
  t=f.split("_")[0]
  print(t)
  df_attributes=pd.read_csv(os.path.join(attributes_folder,f))
  df_attributes_f = df_attributes[~( (df_attributes[annotators_id[0]]=="<no label>") & (df_attributes[annotators_id[1]]=="<no label>"))]
  df_attributes_f = df_attributes_f[~( (df_attributes_f["Flags"]=="INCOMPLETE_POSITION, USED"))]

  agreement_plot(df_attributes_f,annotators_id)

"""# Relax Metrics"""

#load_relation_data (change teh file for other options (curator, ana filipa, ines cantante))

df,ann_ids=getDataandAnn("relations.csv",attribute=False)

#transform the data to segement the spans in new columns

def transformRelationData(df,col="Position"):
  position=df[col]
  begin=list()
  end=list()
  begin2=list()
  end2=list()
  term1=list()
  term2=list()
  for p in position:
    sp=p.split("[")
    term1.append(sp[1].split("]")[0])
    sp=sp[0]
    b,e=sp.split("-")
    begin.append(b[1:])
    end.append(e[:-1])
    sp2=p.split(">")[1]

    term2.append(sp2.split("[")[1][:-1])

    sp2=sp2.split("-")
    b=sp2[0]
    e=sp2[1].split(")")[0]
    #print(sp2)

    begin2.append(b[2:])
    end2.append(e)
  df["span1_begin"]=begin
  df["span1_end"]=end
  df["term1"]=term1
  df["span2_begin"]=begin2
  df["span2_end"]=end2
  df["term2"]=term2

  return df

df=transformRelationData(df)

df

#uncomment if we want to use only incomplete matches
#df=df[~(df["Flags"].isin(['COMPLETE, USED', 'DIFFERENCE, COMPLETE, USED']))]

"""Na verdade só é preciso garantir 3 coisas para o  relax overlap de relações por token



1.   Garantir que há overlap de alguns carateres (1 token de overlap pelo menos)
2.   Ver se a mesma palavra está anotada dos dois lados (não é necessário  visto que o overlap existe)
3.   Garantir que o threshold the tokens não é superior à intersecção dos dois tokens. Para isso ver qual é a substring comum e ver se o len das restantes palavras não é maior que o threshold (len(string)-len(substring comum))<=threshold  


"""

import difflib
#função para contar o número total de tokens extra anotados
def computeExtraWords(ann1_head_term,ann1_tail_term,ann2_head_term,ann2_tail_term):
        total_remaining=0

        for s1,s2 in [(ann1_head_term,ann2_head_term),(ann1_tail_term,ann2_tail_term)]:

          # Find the longest common substring
          matcher = difflib.SequenceMatcher(None, s1, s2)
          match = matcher.find_longest_match(0, len(s1), 0, len(s2))

          # Extract the common substring
          common_substring = s1[match.a: match.a + match.size]

          # Remove the common substring from both strings
          s1_clean = s1.replace(common_substring, '')
          s2_clean = s2.replace(common_substring, '')

          # Split into words
          words1 = s1_clean.split()
          words2 = s2_clean.split()

          # Count words
          total_remaining = total_remaining+ len(words1) + len(words2)
        return total_remaining

def partialMatchWord(annotator1, annotator2, word_threshold=2):
    """
    Compute inter-annotator agreement based on partial match.

    Parameters:
    - annotator1: List of spans from Annotator 1 (each span is a dictionary with 'span1_begin', 'span1_end','span2_begin', 'span2_end',label).
    - annotator2: List of spans from Annotator 2 (each span is a dictionary with 'span1_begin', 'span1_end','span2_begin', 'span2_end',label).
    - word_threshold: Overlap ratio required for two spans to be considered a match (default is 2).

    Returns:
    - dict: Agreement metrics (precision, recall, F1-score, accuracy at different levels).
    """

    def calculate_overlap(span1, span2):
        """Calculate overlap between two spans."""

        #anotador 1
        #begin span 1    begin span 2
        ann1_head_start, ann1_head_end, ann1_tail_start, ann1_tail_end,ann1_head_term,ann1_tail_term,_=span1

        #anotador 2
        #begin span 1    begin span 2
        ann2_head_start, ann2_head_end, ann2_tail_start, ann2_tail_end,ann2_head_term,ann2_tail_term,_=span2

        #span 1 from relation
        overlap_head = max(0, min(ann1_head_end, ann2_head_end) - max(ann1_head_start,ann2_head_start))

        #span 2 from relation
        overlap_tail = max(0, min(ann1_tail_end, ann2_tail_end) - max(ann1_tail_start,ann2_tail_start))

        head_length = max(ann1_head_end-ann1_head_start,ann2_head_end-ann2_head_start)
        tail_length = max(ann1_tail_end-ann1_tail_start,ann2_tail_end-ann2_tail_start)

        return {"overlap_head":overlap_head,"overlap_tail": overlap_tail}


    # Extract spans
    ann1_span = [(span['span1_begin'], span['span1_end'],span['span2_begin'], span['span2_end'],span["term1"],span["term2"],span["span"]) for span in annotator1]
    ann2_span = [(span['span1_begin'], span['span1_end'],span['span2_begin'], span['span2_end'],span["term1"],span["term2"],span["span"]) for span in annotator2]

    # at least one char of threshold (>0)
    char_overlap_threshold=0
    # Find matches based on overlap
    matches_overlap = set()
    matches_overlap_label = set()
    for span1 in ann1_span:
        for span2 in ann2_span:
            #print("-----------")
            #print(span1)
            #print(span2)
            #print("-----------")

            overlap = calculate_overlap(span1, span2)
            extra_words=computeExtraWords(span1[4],span1[5],span2[4],span2[5])

            #print(overlap["overlap_head"],overlap["overlap_tail"],char_overlap_threshold,extra_words,word_threshold,span1[6],span2[6])
            #satisfies the conditions above described
            if overlap["overlap_head"] > char_overlap_threshold and overlap["overlap_tail"] > char_overlap_threshold and extra_words<=word_threshold:
                matches_overlap.add((span1, span2))
                #see if it is the same label
                if(span1[6]==span2[6]):
                  matches_overlap_label.add((span1, span2))

    # Calculate precision, recall, and F1-score
    matched_spans1 = {m[0] for m in matches_overlap}
    matched_spans2 = {m[1] for m in matches_overlap}



    precision = len(matched_spans1) / len(ann1_span) if ann1_span else 0
    recall = len(matched_spans2) / len(ann2_span) if ann2_span else 0
    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    print(matches_overlap)
    print(matched_spans1)
    print(matched_spans2)
    # Return the results
    return {

        "precision": precision,
        "recall": recall,
        "f1_score": f1_score,
        "num_matches": len(matches_overlap),
        "num_matches_label": len(matches_overlap_label),
        "total_annotator1": len(ann1_span),
        "total_annotator2": len(ann2_span),
        "accuracy_matches": len(matches_overlap)/max(len(ann1_span),len(ann2_span)),
        "accuracy_matches_label":len(matches_overlap_label)/max(len(ann1_span),len(ann2_span)),
        "accuracy_matches_label_filtered":len(matches_overlap_label)/len(matches_overlap),

        "threshold": word_threshold
    }

#first match the completes, then go by order of appearance
df.sort_values(by=["Flags","span1_begin","span1_end"],inplace=True)

ann_1_l=list()
ann_2_l=list()
for ind, row in df.iterrows():
  if row[ann_ids[0]]!="<no annotation>":
    #if there is no annotation in annotator 0 appends to annotator 1
    ann_1_l.append({"span1_begin":int(row["span1_begin"]),
                    "span1_end":int(row["span1_end"]),
                    "span2_begin":int(row["span2_begin"]),
                    "span2_end":int(row["span2_end"]),
                    "term1":row["term1"],
                    "term2":row["term2"],
                    "span":row[ann_ids[0]]})

  #if there is no annotation in annotator 1 appends to annotator 2

  if row[ann_ids[1]]!="<no annotation>":
    ann_2_l.append({"span1_begin":int(row["span1_begin"]),
                    "span1_end":int(row["span1_end"]),
                    "span2_begin":int(row["span2_begin"]),
                    "span2_end":int(row["span2_end"]),
                    "term1":row["term1"],
                    "term2":row["term2"],
                    "span":row[ann_ids[1]]})

print(ann_1_l)

partial_match_results=list()
#compute the results for several words thresholds
for t in range(0,10,1):
    r= partialMatchWord(ann_1_l, ann_2_l, word_threshold=t)

    partial_match_results.append(r)

res_partial_df=pd.DataFrame.from_dict(partial_match_results)

res_partial_df.groupby(["threshold"]).mean()

res_partial_df[["threshold","num_matches","num_matches_label","accuracy_matches",	"accuracy_matches_label","accuracy_matches_label_filtered"]].to_csv("relations_relaxed.csv")

