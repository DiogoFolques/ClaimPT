# -*- coding: utf-8 -*-
"""Copy of ClaimPT verbs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FA4042ll4081sdOF6SYn9eUic7I_x7LG
"""

!pip install spacy
!python -m spacy download pt_core_news_md

from google.colab import drive
drive.mount('/content/drive')

import os
import re
import spacy
import pandas as pd


# configurações
base_folder = "/content/drive/MyDrive/ClaimPT/claims_falsas"
topics = ["chega","eleicoes","fake_news","trump"]
verbs2capture = {
    "dizer","alertar","afirmar","salientar","referir","defender",
    "acrescentar","ler","vincar","indicar","frisar",
    "criticar","apontar","destacar","escrever","explicar",
    "lamentar","recordar","sublinhar","sugerir","sustentar",
    "prosseguir","observar","notar","acusar","lembrar","justificar",
    "invocar","garantir","elucidar","considerar","concluir",
    "declarar","argumentar","enfatizar","adiantar"
}

nlp = spacy.load("pt_core_news_md", disable=["parser","ner"])
nlp.add_pipe("sentencizer")

# regex para extrair discurso direto entre aspas (vários tipos)
QUOTE_PATTERN = re.compile(
    r'(?P<open>["“«‘])'      # abrir
    r'([^"“”«»‘’]+?)'        # conteúdo não-guloso
    r'(?P<close>["”»’])'     # fechar
)

def numeric_key(fname):
    m = re.search(r'(\d+)', fname)
    return int(m.group(1)) if m else -1

# função principal

def extract_from_file(filepath, window=5):
    """
    1) Extrai quotes do texto completo (mesmo multi-frase).
    2) Mapeia para indices de tokens globais.
    3) Determina quais frases se sobrepõem ao span citado.
    4) Procura verbos na window antes/depois, mas filtra para
       só permitir verbos dessas mesmas frases.
    """
    text = open(filepath, encoding="utf-8", errors="ignore").read()
    if not text:
      return []

    doc = nlp(text)
    if len(doc) == 0:
      return []

    tokens = list(doc)
    sents = list(doc.sents)
    results = []

    for match in QUOTE_PATTERN.finditer(text):
        quoted = match.group(2).strip()
        if len(quoted.split()) <=6:
            continue

        # caracter spans
        char_start, char_end = match.start(2), match.end(2)
        # token spans (global)
        tok_start = next((i for i, t in enumerate(tokens) if t.idx >= char_start), None)
        tok_end   = next((i for i, t in enumerate(tokens) if t.idx + len(t) > char_end), None)
        if tok_start is None or tok_end is None:
            continue

        # quais frases se sobrepõem a este span?
        overlapping_sents = [
            sent for sent in sents
            if not (sent.end_char < char_start or sent.start_char > char_end)
        ]
        if not overlapping_sents:
            continue

        # construir contexto antes / depois
        before_ctx = tokens[max(0, tok_start - window):tok_start]
        after_ctx  = tokens[tok_end:tok_end + window]

        # filtrar tokens para só aqueles nas frases overlapping
        valid_sents = set(overlapping_sents)
        context = [
            tok for tok in (before_ctx + after_ctx)
            if tok.sent in valid_sents
        ]

        # buscar verbos no contexto filtrado
        for tok in context:
            if tok.pos_ == "VERB":
                if re.search(r'\W+', tok.text):
                  tok_nh=re.split(r'\W+', tok.text)[0]
                  tok=nlp(tok_nh)[0]
                lemma = tok.lemma_.lower()
                if lemma in verbs2capture:
                    results.append({
                        "file":   os.path.basename(filepath),
                        "quoted": quoted,
                        "verbo":  tok
                    })
                    break  # só o primeiro por citação

    return results



# processar cada tópico e gerar CSV
for topic in topics:
    topic_folder = os.path.join(base_folder, topic)
    all_results = []


    for fname in sorted(os.listdir(topic_folder), key=numeric_key):
      if not fname.endswith('.txt'):
          continue
      path = os.path.join(topic_folder, fname)
      try:
          results = extract_from_file(path)
      except IndexError as e:
          print(f"❌  Error processing {path!r}: {e}")
          continue
      all_results.extend(results)


    # guardar CSV específico do tópico
    df = pd.DataFrame(all_results)
    output_path = f"results_{topic}.csv"
    df.to_csv(output_path, index=False)
    print(f"Saved {len(df)} entries to {output_path}")

import matplotlib.pyplot as plt

# Helper to count words in a file
def count_words(filepath):
    text = open(filepath, encoding="utf-8", errors="ignore").read()
    # simple word count: alphanumeric sequences
    return len(re.findall(r'\w+', text))

# Prepare master metrics storage
all_metrics = []

for topic in topics:
    # Read the claims csv for this topic
    df_claims = pd.read_csv(f"results_{topic}.csv")
    # Compute raw counts per article
    raw_counts = df_claims.groupby("file").size().rename("claims_count").reset_index()

    # Get word counts per article by reading original files
    wc_list = []
    topic_folder = os.path.join(base_folder, topic)
    for fname in raw_counts["file"]:
        path = os.path.join(topic_folder, fname)
        wc_list.append(count_words(path))
    raw_counts["word_count"] = wc_list

    # Compute density: claims per 1000 words
    raw_counts["density"] = raw_counts["claims_count"] / raw_counts["word_count"] * 1000

    # Normalization
    raw_counts["norm_count"] = raw_counts["claims_count"] / raw_counts["claims_count"].max()
    raw_counts["norm_density"] = raw_counts["density"] / raw_counts["density"].max()
    raw_counts["combined_score"] = (raw_counts["norm_count"] + raw_counts["norm_density"]) / 2

    # Save top100 rankings
    raw_rank = raw_counts.sort_values("claims_count", ascending=False).head(100)[["file", "claims_count"]]
    den_rank = raw_counts.sort_values("density", ascending=False).head(100)[["file", "density"]]
    comb_rank = raw_counts.sort_values("combined_score", ascending=False).head(100)[["file", "combined_score"]]

    raw_rank.to_csv(f"ranking_raw_{topic}.csv", index=False)
    den_rank.to_csv(f"ranking_density_{topic}.csv", index=False)
    comb_rank.to_csv(f"ranking_combined_{topic}.csv", index=False)

    # Collect for overall distribution
    for _, row in raw_counts.iterrows():
        all_metrics.append({
            "topic": topic,
            "file": row["file"],
            "claims_count": row["claims_count"],
            "density": row["density"],
            "combined_score": row["combined_score"]
        })

# Create master DataFrame
master_df = pd.DataFrame(all_metrics)

# Distribution plot: sorted metric values overall
metrics = ["claims_count", "density", "combined_score"]
plt.figure(figsize=(8, 5))
for metric in metrics:
    sorted_vals = master_df[metric].sort_values(ascending=False).values
    plt.plot(sorted_vals, label=metric)
plt.xlabel("Article Rank (by metric)")
plt.ylabel("Metric Value")
plt.title("Distribution of Claim Metrics Across All Articles")
plt.legend()
plt.tight_layout()
plt.show()

-# Print Top 20 Files by Combined Score for Each Topic

top20_per_topic = {}

for topic in topics:
    # Read the combined ranking CSV
    df_comb = pd.read_csv(f"ranking_combined_{topic}.csv")
    # Get top 20 file names
    top20 = df_comb["file"].head(20).tolist()
    top20_per_topic[topic] = top20

# Print the results
for topic, files in top20_per_topic.items():
    for rank, fname in enumerate(files, start=1):
        print(fname)